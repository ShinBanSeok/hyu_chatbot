import os
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 1. ë²¡í„° DB ë¡œë“œ
persist_directory = "./hyuwiki_vectorstore"
embedding_model = OpenAIEmbeddings()
vectordb = Chroma(
    persist_directory=persist_directory, embedding_function=embedding_model
)

# 2. ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
query = "ë¬´ì‹ ì‚¬ ì–¸ì œ ìƒê²¼ë‚˜ìš”?"
docs = vectordb.similarity_search(query, k=3)

# 3. context ì¶”ì¶œ
context = "\n\n".join([doc.page_content for doc in docs])

# 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë‹¹ì‹ ì€ í•œì–‘ëŒ€í•™êµì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •ë‹µì„ ë‹µí•˜ì„¸ìš”.

ê²€ìƒ‰ ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”:
""",
)

prompt = prompt_template.format(context=context, question=query)

# 5. LLM í˜¸ì¶œ
llm = ChatOpenAI(temperature=0)
response = llm.invoke(prompt)

# 6. ì¶œë ¥
print("ğŸ¤– ë‹µë³€:", response.content)
